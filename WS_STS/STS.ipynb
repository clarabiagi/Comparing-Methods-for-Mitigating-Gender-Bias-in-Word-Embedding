{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!conda install -c conda-forge gdown\n",
    "import numpy as np\n",
    "import scipy, requests, codecs, os, re, nltk, itertools, csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tensorflow as tf\n",
    "from scipy.stats import spearmanr\n",
    "import pandas as pd\n",
    "import functools as ft\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "#import gdown\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_from_np(filename):\n",
    "    print('loading ...')\n",
    "    with codecs.open(filename + '.vocab', 'r', 'utf-8') as f_embed:\n",
    "        vocab = [line.strip() for line in f_embed]\n",
    "        \n",
    "    w2i = {w: i for i, w in enumerate(vocab)}\n",
    "    wv = np.load(filename + '.wv.npy')\n",
    "\n",
    "    return vocab, wv, w2i\n",
    "\n",
    "def load_dhdglove(path):\n",
    "    print('loading ...')\n",
    "    debiased_embeds = pickle.load(open(path, 'rb'))\n",
    "    wv = []\n",
    "    vocab = []\n",
    "    for w in debiased_embeds:\n",
    "        wv.append(np.array(debiased_embeds[w]))\n",
    "        vocab.append(str(w))\n",
    "        \n",
    "    w2i = {w: i for i, w in enumerate(vocab)}\n",
    "    wv = np.array(wv).astype(float)\n",
    "    print(len(vocab), wv.shape, len(w2i))\n",
    "        \n",
    "    return vocab, wv, w2i \n",
    "\n",
    "def load_wo_normalize(space, filename, vocab, wv, w2i):\n",
    "    if filename[-3:]=='txt':\n",
    "        vocab_muse, wv_muse, w2i_muse = load_embeddings_from_np(filename)\n",
    "    else:\n",
    "        vocab_muse, wv_muse, w2i_muse = load_dhdglove(filename)\n",
    "    vocab[space] = vocab_muse \n",
    "    wv[space] = wv_muse\n",
    "    w2i[space] = w2i_muse\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ...\n",
      "done\n",
      "loading ...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "wv = {}\n",
    "w2i = {}\n",
    "\n",
    "load_wo_normalize('bef', 'Gender-Biased Word Relation Task/data/embeddings/glove_wiki_vectors.txt', vocab, wv, w2i)\n",
    "# load_wo_normalize('aft', 'Gender-Biased Word Relation Task/data/embeddings/hsrglove_wiki_vectors.txt', vocab, wv, w2i)\n",
    "load_wo_normalize('aft', 'Gender-Biased Word Relation Task/hsr_ran_glove.txt', vocab, wv, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_glove = dict(zip(vocab['bef'], wv['bef']))\n",
    "post_glove = dict(zip(vocab['aft'], wv['aft']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year-task</th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-MSRvid</td>\n",
       "      <td>A plane is taking off.</td>\n",
       "      <td>An air plane is taking off.</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-MSRvid</td>\n",
       "      <td>A man is playing a large flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-MSRvid</td>\n",
       "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>A man is spreading shredded cheese on an uncoo...</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-MSRvid</td>\n",
       "      <td>Three men are playing chess.</td>\n",
       "      <td>Two men are playing chess.</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-MSRvid</td>\n",
       "      <td>A man is playing the cello.</td>\n",
       "      <td>A man seated is playing the cello.</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     year-task                                         sent_1  \\\n",
       "0  2012-MSRvid                         A plane is taking off.   \n",
       "1  2012-MSRvid                A man is playing a large flute.   \n",
       "2  2012-MSRvid  A man is spreading shreded cheese on a pizza.   \n",
       "3  2012-MSRvid                   Three men are playing chess.   \n",
       "4  2012-MSRvid                    A man is playing the cello.   \n",
       "\n",
       "                                              sent_2   sim  \n",
       "0                        An air plane is taking off.  5.00  \n",
       "1                          A man is playing a flute.  3.80  \n",
       "2  A man is spreading shredded cheese on an uncoo...  3.80  \n",
       "3                         Two men are playing chess.  2.60  \n",
       "4                 A man seated is playing the cello.  4.25  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resourceFile = 'data/' \n",
    "def load_sts_dataset(filename):\n",
    "    # For a STS dataset, loads the relevant information: the sentences and their human rated similarity score.\n",
    "    sent_pairs = []\n",
    "    with tf.io.gfile.GFile(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            ts = line.strip().split(\"\\t\")\n",
    "            if len(ts) == 7 or len(ts) == 9:\n",
    "                sent_pairs.append((re.sub(\"[^0-9]\", \"\", ts[2]) + '-' + ts[1] , ts[5], ts[6], float(ts[4])))\n",
    "            elif len(ts) == 6 or len(ts) == 8:\n",
    "                sent_pairs.append((re.sub(\"[^0-9]\", \"\", ts[1]) + '-' + ts[0] , ts[4], ts[5], float(ts[3])))\n",
    "            else:\n",
    "                print('data format is wrong!!!')\n",
    "    return pd.DataFrame(sent_pairs, columns=[\"year-task\", \"sent_1\", \"sent_2\", \"sim\"])\n",
    "\n",
    "\n",
    "def load_all_sts_dataset():\n",
    "    # Loads all of the STS datasets \n",
    "    stsbenchmarkDir = resourceFile + 'stsbenchmark/'\n",
    "    stscompanionDir = resourceFile + 'stsbenchmark/'\n",
    "    sts_train = load_sts_dataset(os.path.join(stsbenchmarkDir, \"sts-train.csv\"))    \n",
    "    sts_dev = load_sts_dataset(os.path.join(stsbenchmarkDir, \"sts-dev.csv\"))\n",
    "    sts_test = load_sts_dataset(os.path.join(stsbenchmarkDir, \"sts-test.csv\"))\n",
    "    sts_other = load_sts_dataset(os.path.join(stscompanionDir, \"sts-other.csv\"))\n",
    "    sts_mt = load_sts_dataset(os.path.join(stscompanionDir, \"sts-mt.csv\"))\n",
    "    \n",
    "    sts_all = pd.concat([sts_train, sts_dev, sts_test, sts_other, sts_mt ])\n",
    "    \n",
    "    return sts_all\n",
    "\n",
    "sts_all = load_all_sts_dataset()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_sts_by_year_task():\n",
    "    # Divide STS datasets based on their year and tasks\n",
    "    sts_by_year_task = {}\n",
    "    \n",
    "    for year_task in sts_all['year-task'].unique():\n",
    "        indices = [i for i, x in enumerate(list(sts_all['year-task'])) if x == year_task]\n",
    "        \n",
    "        pairs = sts_all.iloc[indices]\n",
    "        \n",
    "        sts_by_year_task[year_task] = pairs\n",
    "        \n",
    "    return sts_by_year_task\n",
    "\n",
    "sts_by_year_task = load_sts_by_year_task()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_sts_by_year():\n",
    "    # Divide STS datasets ONLY based on their year (different tasks in that year are merged).\n",
    "\n",
    "    sts_by_year = {}\n",
    "    \n",
    "    for year in ['2012', '2013', '2014', '2015', '2016', '2017']:\n",
    "        indices = [i for i, x in enumerate(list(sts_all['year-task'])) if x.startswith(year)]\n",
    "        \n",
    "        pairs = sts_all.iloc[indices]\n",
    "        pairs = pairs.copy()\n",
    "        pairs['year-task'] = year\n",
    "        sts_by_year[year] = pairs\n",
    "        \n",
    "    return sts_by_year\n",
    "\n",
    "sts_by_year_task = load_sts_by_year_task()\n",
    "\n",
    "sts_by_year = load_sts_by_year()\n",
    "\n",
    "\n",
    "filename = resourceFile + '2015-answers-students.test.tsv'\n",
    "sent_pairs = []\n",
    "with tf.io.gfile.GFile(filename, \"r\") as f:\n",
    "    for line in f:\n",
    "        ts = line.strip().split(\"\\t\")\n",
    "        if len(ts) == 3:\n",
    "            sent_pairs.append((ts[1], ts[2], float(ts[0])))\n",
    "answers_students_2015 =  pd.DataFrame(sent_pairs, columns=[\"sent_1\", \"sent_2\", \"sim\"])\n",
    "\n",
    "\n",
    "# show some sample sts data    \n",
    "sts_all[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>sim</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>There is no boy playing outdoors and there is ...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>3.300</td>\n",
       "      <td>NEUTRAL\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>A group of boys in a yard is playing and a man...</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>3.700</td>\n",
       "      <td>NEUTRAL\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>A group of children is playing in the house an...</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>3.000</td>\n",
       "      <td>NEUTRAL\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>A brown dog is attacking another animal in fro...</td>\n",
       "      <td>A brown dog is attacking another animal in fro...</td>\n",
       "      <td>4.900</td>\n",
       "      <td>ENTAILMENT\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>A brown dog is attacking another animal in fro...</td>\n",
       "      <td>A brown dog is helping another animal in front...</td>\n",
       "      <td>3.665</td>\n",
       "      <td>NEUTRAL\\r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  idx                                             sent_1  \\\n",
       "0   6  There is no boy playing outdoors and there is ...   \n",
       "1   7  A group of boys in a yard is playing and a man...   \n",
       "2   8  A group of children is playing in the house an...   \n",
       "3  10  A brown dog is attacking another animal in fro...   \n",
       "4  11  A brown dog is attacking another animal in fro...   \n",
       "\n",
       "                                              sent_2    sim         label  \n",
       "0  A group of kids is playing in a yard and an ol...  3.300     NEUTRAL\\r  \n",
       "1  The young boys are playing outdoors and the ma...  3.700     NEUTRAL\\r  \n",
       "2  The young boys are playing outdoors and the ma...  3.000     NEUTRAL\\r  \n",
       "3  A brown dog is attacking another animal in fro...  4.900  ENTAILMENT\\r  \n",
       "4  A brown dog is helping another animal in front...  3.665     NEUTRAL\\r  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def download_sick(f): \n",
    "\n",
    "    response = requests.get(f).text\n",
    "\n",
    "    lines = response.split(\"\\n\")[1:]\n",
    "    lines = [l.split(\"\\t\") for l in lines if len(l) > 0]\n",
    "    lines = [l for l in lines if len(l) == 5]\n",
    "\n",
    "    df = pd.DataFrame(lines, columns=[\"idx\", \"sent_1\", \"sent_2\", \"sim\", \"label\"])\n",
    "    df['sim'] = pd.to_numeric(df['sim'])\n",
    "    return df\n",
    "    \n",
    "sick_all = download_sick(\"https://raw.githubusercontent.com/alvations/stasis/master/SICK-data/SICK_test_annotated.txt\")\n",
    "\n",
    "sick_all[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    \n",
    "    def __init__(self, sentence):\n",
    "        self.raw = sentence\n",
    "        normalized_sentence = sentence.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "        self.tokens = [t.lower() for t in nltk.word_tokenize(normalized_sentence)]\n",
    "        \n",
    "def run_benchmark(sentences1, sentences2, model_str): \n",
    "    \n",
    "    model = eval(model_str)\n",
    "    embeddings = []\n",
    "    \n",
    "    wv_len = 300\n",
    "    \n",
    "    if 'bert' in model_str:\n",
    "        wv_len = 768\n",
    "        \n",
    "    for (sent1, sent2) in zip(sentences1, sentences2): \n",
    "\n",
    "        tokens1 =  sent1.tokens\n",
    "        tokens2 =  sent2.tokens\n",
    "\n",
    "        tokens1 = [token for token in tokens1 if token in model and token.islower()]\n",
    "        tokens2 = [token for token in tokens2 if token in model and token.islower()]\n",
    "        \n",
    "        if tokens1 == [] and tokens2 != []:\n",
    "            embedding1 = np.zeros(wv_len)\n",
    "            embedding2 = np.average([model[token] for token in tokens2], axis=0)\n",
    "        elif tokens2 == [] and tokens1 != []:\n",
    "            embedding2 = np.zeros(wv_len)\n",
    "            embedding1 = np.average([model[token] for token in tokens1], axis=0)\n",
    "        elif tokens2 != [] and tokens1 != []:     \n",
    "            embedding1 = np.average([model[token] for token in tokens1], axis=0)\n",
    "            embedding2 = np.average([model[token] for token in tokens2], axis=0)\n",
    "        else:\n",
    "            embedding1 = np.zeros(wv_len)\n",
    "            embedding2 = np.zeros(wv_len)\n",
    "\n",
    "#         if isinstance(embedding1, float) or isinstance(embedding2, float):\n",
    "#             embeddings.append(np.zeros(300))\n",
    "#             embeddings.append(np.zeros(300))\n",
    "#         else:\n",
    "#             embeddings.append(embedding1)\n",
    "#             embeddings.append(embedding2)\n",
    "        embeddings.append(embedding1)\n",
    "        embeddings.append(embedding2)\n",
    "\n",
    "\n",
    "    sims = [cosine_similarity(embeddings[idx*2].reshape(1, -1), embeddings[idx*2+1].reshape(1, -1))[0][0] for idx in range(int(len(embeddings)/2))]\n",
    "    return sims\n",
    "\n",
    "def run_experiment(df, benchmarks): \n",
    "    \n",
    "    sentences1 = [Sentence(s) for s in df['sent_1']]\n",
    "    sentences2 = [Sentence(s) for s in df['sent_2']]\n",
    "    \n",
    "    pearson_cors, spearman_cors = [], []\n",
    "    for label, method in benchmarks:\n",
    "        sims = method(sentences1, sentences2)\n",
    "        pearson_correlation = round(scipy.stats.pearsonr(sims, df['sim'])[0] * 100,2)\n",
    "        #print(label, pearson_correlation)\n",
    "        pearson_cors.append(pearson_correlation)\n",
    "        \n",
    "    return pearson_cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\biagi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STS-2012-MSRvid\n",
      "STS-2014-images\n",
      "STS-2015-images\n",
      "STS-2014-deft-forum\n",
      "STS-2012-MSRpar\n",
      "STS-2014-deft-news\n",
      "STS-2013-headlines\n",
      "STS-2014-headlines\n",
      "STS-2015-headlines\n",
      "STS-2016-headlines\n",
      "STS-2017-track5.en-en\n",
      "STS-2015-answers-forums\n",
      "STS-2016-answer-answer\n",
      "STS-2012-surprise.OnWN\n",
      "STS-2013-FNWN\n",
      "STS-2013-OnWN\n",
      "STS-2014-OnWN\n",
      "STS-2014-tweet-news\n",
      "STS-2015-belief\n",
      "STS-2016-plagiarism\n",
      "STS-2016-question-question\n",
      "STS-2012-SMTeuroparl\n",
      "STS-2012-surprise.SMTnews\n",
      "STS-2016-postediting\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "benchmarks = [\n",
    "     (\"orig-glove\", ft.partial(run_benchmark, model_str= 'orig_glove')),\n",
    "    (\"HSR-glove\", ft.partial(run_benchmark, model_str= 'post_glove'))]\n",
    "\n",
    "pearson_results_year_task = {}\n",
    "\n",
    "for year_task in sts_all['year-task'].unique():\n",
    "    print('STS-' + year_task)\n",
    "    pearson_results_year_task['STS-' + year_task] = run_experiment(sts_by_year_task[year_task], benchmarks)  \n",
    "    \n",
    "pearson_results_year_task['SICK'] = run_experiment(sick_all, benchmarks) \n",
    "pearson_results_year_task['2015-answers_students'] = run_experiment(answers_students_2015, benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_results_year_task_df = pd.DataFrame(pearson_results_year_task)\n",
    "pearson_results_year_task_df = pearson_results_year_task_df.transpose()\n",
    "pearson_results_year_task_df = pearson_results_year_task_df.rename(columns={i:b[0] for i, b in enumerate(benchmarks)})\n",
    "\n",
    "pearson_results_year_task_df=pearson_results_year_task_df.reindex(['STS-2012-MSRpar', 'STS-2012-MSRvid', 'STS-2012-surprise.OnWN', 'STS-2012-SMTeuroparl', 'STS-2012-surprise.SMTnews','STS-2013-FNWN', 'STS-2013-OnWN', 'STS-2013-headlines',  'STS-2014-OnWN', 'STS-2014-deft-forum','STS-2014-deft-news', 'STS-2014-headlines', 'STS-2014-tweet-news',  'STS-2014-images', 'STS-2015-answers-forums', '2015-answers_students', 'STS-2015-belief',  'STS-2015-headlines', 'STS-2015-images', 'SICK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_results_year_task_df.to_csv('hsr_ran.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pearson_results_year_task_df.to_csv('hsr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_2012 = np.mean(pearson_results_year_task_df.iloc[:5, 0])\n",
    "# orig_2013 = np.mean(pearson_results_year_task_df.iloc[5:8, 0])\n",
    "# orig_2014 = np.mean(pearson_results_year_task_df.iloc[8:14, 0])\n",
    "# orig_2015 = np.mean(pearson_results_year_task_df.iloc[14:19, 0])\n",
    "# orig_SICK = np.mean(pearson_results_year_task_df.iloc[19, 0])\n",
    "# HSR_2012 = np.mean(pearson_results_year_task_df.iloc[:5, 1])\n",
    "# HSR_2013 = np.mean(pearson_results_year_task_df.iloc[5:8, 1])\n",
    "# HSR_2014 = np.mean(pearson_results_year_task_df.iloc[8:14, 1])\n",
    "# HSR_2015 = np.mean(pearson_results_year_task_df.iloc[14:19, 1])\n",
    "# HSR_SICK = np.mean(pearson_results_year_task_df.iloc[19, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_2012 = np.mean(pearson_results_year_task_df.iloc[:5, 0])\n",
    "orig_2013 = np.mean(pearson_results_year_task_df.iloc[5:8, 0])\n",
    "orig_2014 = np.mean(pearson_results_year_task_df.iloc[8:14, 0])\n",
    "orig_2015 = np.mean(pearson_results_year_task_df.iloc[14:19, 0])\n",
    "orig_SICK = np.mean(pearson_results_year_task_df.iloc[19, 0])\n",
    "HSRRAN_2012 = np.mean(pearson_results_year_task_df.iloc[:5, 1])\n",
    "HSRRAN_2013 = np.mean(pearson_results_year_task_df.iloc[5:8, 1])\n",
    "HSRRAN_2014 = np.mean(pearson_results_year_task_df.iloc[8:14, 1])\n",
    "HSRRAN_2015 = np.mean(pearson_results_year_task_df.iloc[14:19, 1])\n",
    "HSRRAN_SICK = np.mean(pearson_results_year_task_df.iloc[19, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_glove</th>\n",
       "      <th>HSR-RAN_glove</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>48.918000</td>\n",
       "      <td>49.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>46.896667</td>\n",
       "      <td>49.673333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>51.023333</td>\n",
       "      <td>53.871667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>51.350000</td>\n",
       "      <td>53.418000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SICK</th>\n",
       "      <td>62.110000</td>\n",
       "      <td>61.980000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      orig_glove  HSR-RAN_glove\n",
       "2012   48.918000      49.960000\n",
       "2013   46.896667      49.673333\n",
       "2014   51.023333      53.871667\n",
       "2015   51.350000      53.418000\n",
       "SICK   62.110000      61.980000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results_STS = {'orig_glove':[orig_2012, orig_2013, orig_2014, orig_2015, orig_SICK], 'HSR_glove':[HSR_2012, HSR_2013, HSR_2014, HSR_2015, HSR_SICK]}\n",
    "results_STS = {'orig_glove':[orig_2012, orig_2013, orig_2014, orig_2015, orig_SICK], 'HSR-RAN_glove':[HSRRAN_2012, HSRRAN_2013, HSRRAN_2014, HSRRAN_2015, HSRRAN_SICK]}\n",
    "results_STS = pd.DataFrame(results_STS)\n",
    "results_STS.index = ['2012', '2013', '2014', '2015', 'SICK']\n",
    "results_STS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def load_embeddings_from_np(filename):\n",
    "    print('loading ...')\n",
    "    with codecs.open(filename + '.vocab', 'r', 'utf-8') as f_embed:\n",
    "        vocab = [line.strip() for line in f_embed]\n",
    "        \n",
    "    w2i = {w: i for i, w in enumerate(vocab)}\n",
    "    wv = np.load(filename + '.wv.npy')\n",
    "\n",
    "    return vocab, wv, w2i\n",
    "\n",
    "def load_dhdglove(path):\n",
    "    print('loading ...')\n",
    "    debiased_embeds = pickle.load(open(path, 'rb'))\n",
    "    wv = []\n",
    "    vocab = []\n",
    "    for w in debiased_embeds:\n",
    "        wv.append(np.array(debiased_embeds[w]))\n",
    "        vocab.append(str(w))\n",
    "        \n",
    "    w2i = {w: i for i, w in enumerate(vocab)}\n",
    "    wv = np.array(wv).astype(float)\n",
    "    print(len(vocab), wv.shape, len(w2i))\n",
    "        \n",
    "    return vocab, wv, w2i \n",
    "\n",
    "def load_wo_normalize(space, filename, vocab, wv, w2i):\n",
    "    if filename[-3:]=='txt':\n",
    "        vocab_muse, wv_muse, w2i_muse = load_embeddings_from_np(filename)\n",
    "    else:\n",
    "        vocab_muse, wv_muse, w2i_muse = load_dhdglove(filename)\n",
    "    vocab[space] = vocab_muse \n",
    "    wv[space] = wv_muse\n",
    "    w2i[space] = w2i_muse\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ...\n",
      "done\n",
      "loading ...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "wv = {}\n",
    "w2i = {}\n",
    "\n",
    "load_wo_normalize('bef', 'Gender-Biased Word Relation Task/data/embeddings/glove_wiki_vectors.txt', vocab, wv, w2i)\n",
    "load_wo_normalize('aft', 'Gender-Biased Word Relation Task/data/embeddings/vectors_hd.txt', vocab, wv, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_glove = dict(zip(vocab['bef'], wv['bef']))\n",
    "post_glove = dict(zip(vocab['aft'], wv['aft']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSets = ['EN-RG-65.txt', 'EN-WS-353-ALL.txt', 'EN-RW-STANFORD.txt', 'EN-MEN-TR-3k.txt', 'EN-MTurk-287.txt', 'EN-MTurk-771.txt', 'EN-SIMLEX-999.txt', 'EN-SimVerb-3500.txt']\n",
    "\n",
    "\n",
    "def similarity_eval(dataSetAddress, wordVecModel_str):\n",
    "    wordVecModel = eval(wordVecModel_str)\n",
    "    vocab = set(list(wordVecModel.keys()))\n",
    "    \n",
    "    fread_simlex = open(dataSetAddress, \"r\")\n",
    "    \n",
    "    pair_list = []\n",
    "\n",
    "    line_number = 0\n",
    "    for line in fread_simlex:\n",
    "#         if line_number > 0:\n",
    "        tokens = line.split()\n",
    "        word_i = tokens[0]\n",
    "        word_j = tokens[1]\n",
    "        score = float(tokens[2])\n",
    "        if word_i in vocab and word_j in vocab:\n",
    "            pair_list.append( ((word_i, word_j), score) )\n",
    "#         line_number += 1\n",
    "\n",
    "    pair_list.sort(key=lambda x: - x[1]) # order the pairs from highest score (most similar) to lowest score (least similar)\n",
    "\n",
    "\n",
    "    extracted_scores = {}\n",
    "\n",
    "    extracted_list = []\n",
    "    \n",
    "               \n",
    "    for (x,y) in pair_list:\n",
    "        (word_i, word_j) = x\n",
    "        \n",
    "        current_distance = 1- cosine_similarity( wordVecModel[word_i].reshape(1,-1)  , wordVecModel[word_j].reshape(1,-1) )        \n",
    "\n",
    "        extracted_scores[(word_i, word_j)] = current_distance\n",
    "        extracted_list.append(((word_i, word_j), current_distance))\n",
    "\n",
    "    extracted_list.sort(key=lambda x: x[1])\n",
    "\n",
    "    spearman_original_list = []\n",
    "    spearman_target_list = []\n",
    "\n",
    "    for position_1, (word_pair, score_1) in enumerate(pair_list):\n",
    "        score_2 = extracted_scores[word_pair]\n",
    "        position_2 = extracted_list.index((word_pair, score_2))\n",
    "        spearman_original_list.append(position_1)\n",
    "        spearman_target_list.append(position_2)\n",
    "\n",
    "    spearman_rho = spearmanr(spearman_original_list, spearman_target_list)\n",
    "    \n",
    "    return spearman_rho[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating the data set EN-RG-65.txt\n",
      "Glove + Orig : 0.7540\n",
      "Glove + HD : 0.7648 \n",
      "\n",
      "evaluating the data set EN-WS-353-ALL.txt\n",
      "Glove + Orig : 0.6199\n",
      "Glove + HD : 0.6207 \n",
      "\n",
      "evaluating the data set EN-RW-STANFORD.txt\n",
      "Glove + Orig : 0.3722\n",
      "Glove + HD : 0.3720 \n",
      "\n",
      "evaluating the data set EN-MEN-TR-3k.txt\n",
      "Glove + Orig : 0.7216\n",
      "Glove + HD : 0.7212 \n",
      "\n",
      "evaluating the data set EN-MTurk-287.txt\n",
      "Glove + Orig : 0.6480\n",
      "Glove + HD : 0.6468 \n",
      "\n",
      "evaluating the data set EN-MTurk-771.txt\n",
      "Glove + Orig : 0.6486\n",
      "Glove + HD : 0.6504 \n",
      "\n",
      "evaluating the data set EN-SIMLEX-999.txt\n",
      "Glove + Orig : 0.3474\n",
      "Glove + HD : 0.3501 \n",
      "\n",
      "evaluating the data set EN-SimVerb-3500.txt\n",
      "Glove + Orig : 0.2038\n",
      "Glove + HD : 0.2034 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "resourceFile = 'data/' \n",
    "\n",
    "for dataset in dataSets:\n",
    "    dataSetAddress = resourceFile + 'wordSimData/' +  dataset\n",
    "    print('evaluating the data set', dataset)\n",
    "    print('Glove + Orig : %.4f' %  similarity_eval(dataSetAddress, 'orig_glove'))\n",
    "    print('Glove + HD : %.4f' %  similarity_eval(dataSetAddress, 'post_glove'),'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

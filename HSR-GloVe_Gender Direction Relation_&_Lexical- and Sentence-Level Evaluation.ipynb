{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "!conda install -c conda-forge gdown\n",
    "import numpy as np\n",
    "import scipy, requests, codecs, os, re, nltk, itertools, csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tensorflow as tf\n",
    "from scipy.stats import spearmanr\n",
    "import pandas as pd\n",
    "import functools as ft\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadWordVecs(model_str):\n",
    "    word_dictionary = {}\n",
    "    \n",
    "    input_file_destination = model_str \n",
    "\n",
    "    f = codecs.open(input_file_destination, 'r', 'utf-8') \n",
    "    x = 0\n",
    "    \n",
    "    count = 0\n",
    "    for line in f:\n",
    "        count +=1\n",
    "        line = line.split(\" \", 1)\n",
    "        if len(line) != 2:\n",
    "            print(count)\n",
    "            continue\n",
    "        transformed_key = line[0]\n",
    "\n",
    "        try:\n",
    "            transformed_key = str(transformed_key)\n",
    "\n",
    "        except:\n",
    "            print(\"Can't convert the key to unicode:\", transformed_key)\n",
    "\n",
    "        word_dictionary[transformed_key] = np.fromstring(line[1], dtype=\"float32\", sep=\" \")\n",
    "\n",
    "        if word_dictionary[transformed_key].shape[0] != 300 and x == 0:\n",
    "            print(transformed_key, word_dictionary[transformed_key].shape)\n",
    "            x += 1\n",
    "\n",
    "    return  word_dictionary     \n",
    "# RAN\n",
    "orig_glove = loadWordVecs('RAN-Debias-master/Gender-Biased Word Relation Task/data/embeddings/RAN-GloVe.txt')\n",
    "\n",
    "#HD\n",
    "#orig_glove = loadWordVecs ( 'debiaswe-master/Gender-Biased Word Relation Task/data/embeddings/vectors_hd.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322636"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(orig_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the',\n",
       " array([ 6.90155402e-02, -7.78450891e-02, -9.91548994e-04, -5.26966453e-02,\n",
       "        -1.75126344e-01, -6.04001880e-02, -3.69504443e-03, -3.78684420e-03,\n",
       "        -1.65121648e-02, -3.74004133e-02, -1.99015513e-02,  5.74485660e-02,\n",
       "        -3.36164944e-02, -2.60468549e-03,  5.02324514e-02,  1.74128152e-02,\n",
       "        -4.31678295e-02, -1.68794449e-02,  1.46234063e-02,  1.79833788e-02,\n",
       "        -1.87147502e-02,  5.26954867e-02,  4.26416658e-02,  3.62997726e-02,\n",
       "        -7.88607895e-02,  7.52407461e-02,  2.53498387e-02,  5.15517518e-02,\n",
       "        -4.12075082e-03, -2.53203884e-02, -6.42554536e-02, -5.82898520e-02,\n",
       "        -6.19459376e-02,  6.05986314e-03,  3.42147313e-02,  3.07430252e-02,\n",
       "         4.59513888e-02,  9.59441252e-03,  3.95397283e-02, -5.25809191e-02,\n",
       "        -1.11933306e-01,  1.54392021e-02,  8.45165551e-02, -1.48142995e-02,\n",
       "        -2.65095159e-02,  1.85908824e-02,  6.76562358e-03,  1.15826372e-02,\n",
       "        -3.44865695e-02,  2.90819653e-03, -7.18153790e-02,  2.22873576e-02,\n",
       "         2.11769529e-02, -3.31127048e-02, -4.60370444e-02, -4.34750365e-03,\n",
       "         6.44432148e-03, -3.91849142e-04,  4.77876514e-02,  3.48024257e-02,\n",
       "         3.62181440e-02,  3.31119867e-04,  1.38846301e-02, -1.06125660e-01,\n",
       "        -4.95567545e-02,  2.71851588e-02,  4.14218083e-02,  1.50383748e-02,\n",
       "         3.60907428e-02,  8.78990367e-02,  1.93994213e-02, -2.37826556e-02,\n",
       "         2.55447105e-02,  3.25492881e-02, -2.01748442e-02, -8.83670244e-03,\n",
       "        -1.76493060e-02, -3.24931182e-03,  1.37116425e-02, -7.92314019e-03,\n",
       "        -7.90562630e-02, -1.93860754e-02,  1.92903042e-01,  5.04430495e-02,\n",
       "        -3.97977494e-02, -4.06913199e-02,  1.74239948e-02,  5.88704348e-02,\n",
       "         4.12291847e-02,  3.53434845e-03, -1.87667180e-02, -4.18209424e-03,\n",
       "        -3.49778496e-03,  9.71931685e-03,  2.31186487e-02, -3.12225781e-02,\n",
       "        -5.32222278e-02,  2.65749376e-02,  4.51069847e-02, -4.29620147e-02,\n",
       "         1.09174587e-02, -4.07570228e-02,  3.42323035e-02, -3.44396988e-03,\n",
       "         6.26601726e-02,  4.67988029e-02,  1.18494630e-01, -3.85906324e-02,\n",
       "        -1.28041610e-01,  3.31475735e-02,  2.19055060e-02,  6.25537485e-02,\n",
       "        -1.49771301e-02, -2.76163500e-02, -3.75477626e-04, -7.08282366e-03,\n",
       "         6.96791485e-02,  1.14659546e-02, -1.97729506e-02, -5.76371104e-02,\n",
       "        -1.16676390e-02,  2.31887326e-02, -1.88014880e-02,  1.99716482e-02,\n",
       "        -4.02343972e-03,  2.35562325e-02,  3.93798985e-02, -1.42607680e-02,\n",
       "         1.91356782e-02,  1.73142049e-02,  4.43035141e-02,  4.76383194e-02,\n",
       "         1.49683282e-02, -1.09470719e-02,  2.63570175e-02, -3.12152430e-02,\n",
       "         3.60552333e-02, -2.01749336e-02,  3.77129205e-02, -3.15992162e-02,\n",
       "         2.41827145e-02,  8.32594465e-03,  2.76728012e-02, -6.61799451e-03,\n",
       "        -2.18025595e-02, -8.43140036e-02,  1.19536044e-02,  4.51317392e-02,\n",
       "        -4.84050624e-03,  1.23784825e-01, -3.30650136e-02, -2.11424232e-02,\n",
       "         9.85930040e-02, -5.69608398e-02,  4.01375480e-02, -3.00264824e-02,\n",
       "         8.65964964e-03,  3.05577330e-02,  5.16053997e-02,  4.86539416e-02,\n",
       "         6.37765974e-02,  1.75213758e-02,  2.62362231e-03, -7.33141042e-03,\n",
       "         7.36876810e-03, -8.12649820e-03, -3.54879163e-02,  7.92837236e-03,\n",
       "         5.46347722e-02, -3.96334566e-02, -2.19485606e-03,  5.95813431e-02,\n",
       "         1.81934927e-02,  5.93579561e-02, -4.21892330e-02,  9.35288891e-03,\n",
       "        -1.09740682e-02, -1.23086814e-02,  4.32179868e-02,  5.18769398e-02,\n",
       "         4.00511548e-02, -6.57630190e-02, -4.46027145e-02,  1.59281362e-02,\n",
       "         2.92120385e-03, -5.99499404e-01,  6.26131147e-02,  1.19597204e-02,\n",
       "        -3.83062288e-02, -1.01258885e-02, -3.90849300e-02,  2.32106913e-02,\n",
       "         9.51618403e-02, -6.71711862e-02, -7.50464201e-02, -1.05968185e-01,\n",
       "        -9.71461311e-02, -8.20766855e-03,  1.26633011e-02, -2.18690895e-02,\n",
       "        -1.57961305e-02, -5.09519577e-02,  1.44311890e-01,  1.05751036e-02,\n",
       "         6.25020713e-02,  5.14383689e-02, -4.41971272e-02,  4.24561277e-02,\n",
       "        -5.63087910e-02,  2.01550908e-02, -4.80224611e-03, -6.45969957e-02,\n",
       "        -1.44417873e-02, -8.67578946e-03, -4.15421985e-02,  4.09879349e-02,\n",
       "         1.19930822e-02,  4.60761935e-02,  4.04806472e-02, -2.02372540e-02,\n",
       "        -3.36016417e-02, -1.81767307e-02, -7.78404325e-02,  4.47655842e-02,\n",
       "         3.87841314e-02, -2.68286895e-02, -3.79778721e-05,  2.03926768e-02,\n",
       "         4.17490564e-02,  3.87605429e-02,  3.47900912e-02, -8.79310742e-02,\n",
       "         1.71996523e-02,  5.05157672e-02,  7.12546930e-02,  1.53906900e-03,\n",
       "         2.85340264e-03, -7.66810998e-02, -1.00928396e-02,  1.05242074e-01,\n",
       "         1.69781465e-02, -1.68670304e-02, -2.53486377e-03,  3.78944166e-02,\n",
       "        -5.35232387e-03,  9.49227810e-02, -4.59187925e-02,  8.86445716e-02,\n",
       "        -2.02684384e-02, -5.36884256e-02, -2.02193744e-02,  3.78118604e-02,\n",
       "        -9.52186957e-02, -2.32601892e-02, -5.70874475e-02, -6.08537942e-02,\n",
       "         1.68688763e-02, -7.22688576e-03,  1.73281673e-02, -3.77691258e-03,\n",
       "        -6.13710470e-03, -3.62291224e-02, -1.60925891e-02,  1.15608033e-02,\n",
       "        -5.94747029e-02, -2.40559056e-02, -1.22798393e-02,  8.04298073e-02,\n",
       "        -8.43799766e-03, -4.77813147e-02,  1.83826173e-03,  4.06562490e-03,\n",
       "        -8.66754130e-02,  7.18509108e-02,  6.31974195e-04, -1.51407132e-02,\n",
       "        -1.46995327e-02,  2.84638871e-02,  7.87851773e-03, -2.21406762e-02,\n",
       "         7.89390206e-02, -5.68864234e-02, -5.61445542e-02, -4.40872535e-02,\n",
       "        -1.45041961e-02, -3.20848897e-02,  5.43642975e-02, -2.74635050e-02,\n",
       "        -5.71555272e-03,  4.84973639e-02,  4.19155620e-02,  5.51126301e-02,\n",
       "         6.25976501e-03,  1.85272153e-02, -2.02373024e-02, -2.84872670e-02,\n",
       "         5.56875765e-03, -1.46043664e-02,  7.58245885e-02, -6.75007515e-03],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter((orig_glove.items())) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#only for RAN\n",
    "del orig_glove['322636']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load gender words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_word = []\n",
    "with open('GenderBiasHSR-master/data/female_word_file.txt', \"r+\", encoding='utf8') as f_in:\n",
    "    for line in f_in:\n",
    "        female_word.append(line.replace('\\n',''))   \n",
    "\n",
    "male_word = []\n",
    "with open('GenderBiasHSR-master/data/male_word_file.txt', \"r+\", encoding='utf8') as f_in:\n",
    "    for line in f_in:\n",
    "        male_word.append(line.replace('\\n','')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate gender direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_direction = orig_glove['he'] - orig_glove['she']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.610652e-09]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some examples in the paper\n",
    "\n",
    "cosine_similarity(orig_glove['nurse'].reshape(1,-1), gender_direction.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.7110218e-08]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(orig_glove['colonel'].reshape(1,-1), gender_direction.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.1358119e-08]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(orig_glove['tree'].reshape(1,-1), gender_direction.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17111513]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(orig_glove['dancer'].reshape(1,-1),orig_glove['nurse'].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08879358]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(orig_glove['dancer'].reshape(1,-1), orig_glove['colonel'].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender-bias word relation tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender-definition and non-gender-definition words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_list = female_word + male_word\n",
    "nongender_list = list(set(orig_glove.keys() ) - set(gender_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_wordvec_mat(wordVecModel_str, wordList):\n",
    "    \n",
    "    wordvecDict = eval(wordVecModel_str)\n",
    "    \n",
    "    feasibleWordList = list(set(wordvecDict.keys()) & set(wordList))\n",
    "        \n",
    "    x_collector = []\n",
    "    newDict = {}\n",
    "    for word in feasibleWordList:\n",
    "        x_collector.append(wordvecDict[word])\n",
    "        newDict[word] = wordvecDict[word][:]        \n",
    "                        \n",
    "    x_collector = np.array(x_collector).T    \n",
    "    \n",
    "    return newDict, x_collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, GenderVecs_glove = ensemble_wordvec_mat('orig_glove', gender_list)\n",
    "nonGenderDict_glove, nonGenderVecs_glove = ensemble_wordvec_mat('orig_glove', nongender_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Half-Sibling Regression GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Half_Sibling_Regression(GenderVecs, nonGenderVecs, nonGenderDict):\n",
    "    alpha = 60 # ridge regression parameter\n",
    "    \n",
    "    W = np.linalg.inv(GenderVecs.T @ GenderVecs + alpha * np.eye(GenderVecs.shape[1])) @ GenderVecs.T @ nonGenderVecs\n",
    "    W = np.array(W)\n",
    "    \n",
    "    prediction = GenderVecs @ W\n",
    "    \n",
    "    post_nonGenderVecs = nonGenderVecs  - prediction # modify those non-stop words\n",
    "\n",
    "    post_nonGenderDict = nonGenderDict.copy() # copy the dictionary of non-stop words\n",
    "    \n",
    "    keys = list(post_nonGenderDict.keys())\n",
    "    for i in range(0,len(keys)):\n",
    "        post_nonGenderDict[keys[i]] = post_nonGenderVecs[:, i] # update the modified non-stop words\n",
    "    \n",
    "    \n",
    "    return post_nonGenderDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_nonGenderDict_glove = Half_Sibling_Regression(GenderVecs_glove, nonGenderVecs_glove, nonGenderDict_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the modified non-gender-definition words back to the dataset with gender-definition words\n",
    "    \n",
    "post_glove = orig_glove.copy()\n",
    "\n",
    "for w in post_nonGenderDict_glove.keys():\n",
    "    post_glove[w] = post_nonGenderDict_glove[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig:  [[7.610652e-09]]\n",
      "Post:  [[0.00995467]]\n"
     ]
    }
   ],
   "source": [
    "test_word = 'nurse'\n",
    "\n",
    "print('Orig: ', cosine_similarity(orig_glove[test_word].reshape(1,-1), gender_direction.reshape(1,-1)))\n",
    "print('Post: ', cosine_similarity(post_glove[test_word].reshape(1,-1), gender_direction.reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to hsr_hd_glove.txt\n"
     ]
    }
   ],
   "source": [
    "# save word vector\n",
    "def save_wv(word_vector_str):\n",
    "    \n",
    "    word_dictionary = eval(word_vector_str)\n",
    "    \n",
    "    ListWords = list(word_dictionary.keys())\n",
    "\n",
    "#     print('writing to', 'hsrglove_wiki_vectors.txt')\n",
    "     print('writing to', 'hsr_ran_glove.txt')\n",
    "#    print('writing to', 'hsr_hd_glove.txt')\n",
    "\n",
    "#     with open('hsrglove_wiki_vectors.txt', 'a', encoding = 'utf8') as the_file:\n",
    "     with open('hsr_ran_glove.txt', 'a', encoding = 'utf8') as the_file:\n",
    "#    with open('debiaswe-master/Gender-Biased Word Relation Task/data/embeddings/hsr_hd_glove.txt', 'a', encoding = 'utf8') as the_file:\n",
    "        for word in ListWords:\n",
    "\n",
    "            wordVec = word_dictionary[word]\n",
    "            wordVecString = \" \".join(str(x) for x in wordVec)\n",
    "\n",
    "            the_file.write(word + ' ' + wordVecString  + '\\n')\n",
    "\n",
    "save_wv('post_glove')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Direction Relation Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-by-projection is calculated in \"Gender-Biased Word Relation Task/source/remaining_bias_HSR.ipynb\"(utilizing the code by Gonen and Goldberg 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SemBias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/SemBias.txt'\n",
    "\n",
    "def read_SemBias(filename):\n",
    "    file_read = open(filename, \"r\", encoding = 'utf8')\n",
    "    \n",
    "    SemBias_list = []\n",
    "    \n",
    "    for line in file_read:\n",
    "        pairs = line.rstrip().split('\\t')\n",
    "        \n",
    "        line_temp = []\n",
    "        for p in pairs:\n",
    "            a, b = p.split(':')\n",
    "            line_temp.append([a,b])\n",
    "        \n",
    "        SemBias_list.append(line_temp)\n",
    "    \n",
    "    return SemBias_list\n",
    "\n",
    "SemBias_task = read_SemBias(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_top(task, wordVecModel_str):\n",
    "    wordVecModel = eval(wordVecModel_str)\n",
    "    #vocab = set(list(wordVecModel.keys()))\n",
    "    result_list = []\n",
    "    he_we = wordVecModel['he'].reshape(1,-1)\n",
    "    she_we = wordVecModel['she'].reshape(1,-1)\n",
    "    \n",
    "    for line in task:\n",
    "        temp_score = []\n",
    "        \n",
    "        if len(line) != 4:\n",
    "            print('error')\n",
    "            \n",
    "        for pair in line:\n",
    "            (word_i, word_j) = pair\n",
    "            current_distance = cosine_similarity(he_we - she_we , wordVecModel[word_i].reshape(1,-1) - wordVecModel[word_j].reshape(1,-1) )        \n",
    "            temp_score.append(current_distance)\n",
    "        \n",
    "        result_list.append(temp_score.index(max(temp_score)))\n",
    "        \n",
    "    return result_list\n",
    "\n",
    "xx_orig = eval_top(SemBias_task, 'orig_glove')\n",
    "xx_hs = eval_top(SemBias_task, 'post_glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig:\n",
      "SemBias:  0.8022727272727272 SemBias (subset):  0.575\n",
      "Half-Sibling Regression:\n",
      "SemBias:  0.8590909090909091 SemBias (subset):  0.1\n"
     ]
    }
   ],
   "source": [
    "list_xx = [xx_orig, xx_hs]\n",
    "\n",
    "print('Orig:')\n",
    "print('SemBias: ',end =\" \")\n",
    "print(list_xx[0].count(0)/440,end =\" \")\n",
    "print('SemBias (subset): ',end =\" \")\n",
    "print(list_xx[0][-40:].count(0)/40)\n",
    "      \n",
    "print('Half-Sibling Regression:')\n",
    "print('SemBias: ',end =\" \")\n",
    "print(list_xx[1].count(0)/440,end =\" \")\n",
    "print('SemBias (subset): ',end =\" \")\n",
    "print(list_xx[1][-40:].count(0)/40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical- and Sentence-Level Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSets = ['EN-RG-65.txt', 'EN-WS-353-ALL.txt', 'EN-RW-STANFORD.txt', 'EN-MEN-TR-3k.txt', 'EN-MTurk-287.txt', 'EN-MTurk-771.txt', 'EN-SIMLEX-999.txt', 'EN-SimVerb-3500.txt']\n",
    "\n",
    "\n",
    "\n",
    "def similarity_eval(dataSetAddress, wordVecModel_str):\n",
    "    wordVecModel = eval(wordVecModel_str)\n",
    "    vocab = set(list(wordVecModel.keys()))\n",
    "    \n",
    "    fread_simlex = open(dataSetAddress, \"r\")\n",
    "    \n",
    "    pair_list = []\n",
    "\n",
    "    line_number = 0\n",
    "    for line in fread_simlex:\n",
    "#         if line_number > 0:\n",
    "        tokens = line.split()\n",
    "        word_i = tokens[0]\n",
    "        word_j = tokens[1]\n",
    "        score = float(tokens[2])\n",
    "        if word_i in vocab and word_j in vocab:\n",
    "            pair_list.append( ((word_i, word_j), score) )\n",
    "#         line_number += 1\n",
    "\n",
    "    pair_list.sort(key=lambda x: - x[1]) # order the pairs from highest score (most similar) to lowest score (least similar)\n",
    "\n",
    "\n",
    "    extracted_scores = {}\n",
    "\n",
    "    extracted_list = []\n",
    "    \n",
    "               \n",
    "    for (x,y) in pair_list:\n",
    "        (word_i, word_j) = x\n",
    "        \n",
    "        current_distance = 1- cosine_similarity( wordVecModel[word_i].reshape(1,-1)  , wordVecModel[word_j].reshape(1,-1) )        \n",
    "\n",
    "        extracted_scores[(word_i, word_j)] = current_distance\n",
    "        extracted_list.append(((word_i, word_j), current_distance))\n",
    "\n",
    "    extracted_list.sort(key=lambda x: x[1])\n",
    "\n",
    "    spearman_original_list = []\n",
    "    spearman_target_list = []\n",
    "\n",
    "    for position_1, (word_pair, score_1) in enumerate(pair_list):\n",
    "        score_2 = extracted_scores[word_pair]\n",
    "        position_2 = extracted_list.index((word_pair, score_2))\n",
    "        spearman_original_list.append(position_1)\n",
    "        spearman_target_list.append(position_2)\n",
    "\n",
    "    spearman_rho = spearmanr(spearman_original_list, spearman_target_list)\n",
    "    \n",
    "    return spearman_rho[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating the data set EN-RG-65.txt\n",
      "Glove + Orig : 0.7540\n",
      "Glove + HSR : 0.7764 \n",
      "\n",
      "evaluating the data set EN-WS-353-ALL.txt\n",
      "Glove + Orig : 0.6199\n",
      "Glove + HSR : 0.6554 \n",
      "\n",
      "evaluating the data set EN-RW-STANFORD.txt\n",
      "Glove + Orig : 0.3722\n",
      "Glove + HSR : 0.3868 \n",
      "\n",
      "evaluating the data set EN-MEN-TR-3k.txt\n",
      "Glove + Orig : 0.7216\n",
      "Glove + HSR : 0.7353 \n",
      "\n",
      "evaluating the data set EN-MTurk-287.txt\n",
      "Glove + Orig : 0.6480\n",
      "Glove + HSR : 0.6335 \n",
      "\n",
      "evaluating the data set EN-MTurk-771.txt\n",
      "Glove + Orig : 0.6486\n",
      "Glove + HSR : 0.6652 \n",
      "\n",
      "evaluating the data set EN-SIMLEX-999.txt\n",
      "Glove + Orig : 0.3474\n",
      "Glove + HSR : 0.3971 \n",
      "\n",
      "evaluating the data set EN-SimVerb-3500.txt\n",
      "Glove + Orig : 0.2038\n",
      "Glove + HSR : 0.2635 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "resourceFile = 'data/' \n",
    "\n",
    "for dataset in dataSets:\n",
    "    dataSetAddress = resourceFile + 'wordSimData/' +  dataset\n",
    "    print('evaluating the data set', dataset)\n",
    "    print('Glove + Orig : %.4f' %  similarity_eval(dataSetAddress, 'orig_glove'))\n",
    "    print('Glove + HSR : %.4f' %  similarity_eval(dataSetAddress, 'post_glove'),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year-task</th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-MSRvid</td>\n",
       "      <td>A plane is taking off.</td>\n",
       "      <td>An air plane is taking off.</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-MSRvid</td>\n",
       "      <td>A man is playing a large flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-MSRvid</td>\n",
       "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>A man is spreading shredded cheese on an uncoo...</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-MSRvid</td>\n",
       "      <td>Three men are playing chess.</td>\n",
       "      <td>Two men are playing chess.</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-MSRvid</td>\n",
       "      <td>A man is playing the cello.</td>\n",
       "      <td>A man seated is playing the cello.</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     year-task                                         sent_1  \\\n",
       "0  2012-MSRvid                         A plane is taking off.   \n",
       "1  2012-MSRvid                A man is playing a large flute.   \n",
       "2  2012-MSRvid  A man is spreading shreded cheese on a pizza.   \n",
       "3  2012-MSRvid                   Three men are playing chess.   \n",
       "4  2012-MSRvid                    A man is playing the cello.   \n",
       "\n",
       "                                              sent_2   sim  \n",
       "0                        An air plane is taking off.  5.00  \n",
       "1                          A man is playing a flute.  3.80  \n",
       "2  A man is spreading shredded cheese on an uncoo...  3.80  \n",
       "3                         Two men are playing chess.  2.60  \n",
       "4                 A man seated is playing the cello.  4.25  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resourceFile = 'data/' \n",
    "def load_sts_dataset(filename):\n",
    "    # For a STS dataset, loads the relevant information: the sentences and their human rated similarity score.\n",
    "    sent_pairs = []\n",
    "    with tf.io.gfile.GFile(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            ts = line.strip().split(\"\\t\")\n",
    "            if len(ts) == 7 or len(ts) == 9:\n",
    "                sent_pairs.append((re.sub(\"[^0-9]\", \"\", ts[2]) + '-' + ts[1] , ts[5], ts[6], float(ts[4])))\n",
    "            elif len(ts) == 6 or len(ts) == 8:\n",
    "                sent_pairs.append((re.sub(\"[^0-9]\", \"\", ts[1]) + '-' + ts[0] , ts[4], ts[5], float(ts[3])))\n",
    "            else:\n",
    "                print('data format is wrong!!!')\n",
    "    return pd.DataFrame(sent_pairs, columns=[\"year-task\", \"sent_1\", \"sent_2\", \"sim\"])\n",
    "\n",
    "\n",
    "def load_all_sts_dataset():\n",
    "    # Loads all of the STS datasets \n",
    "    stsbenchmarkDir = resourceFile + 'stsbenchmark/'\n",
    "    stscompanionDir = resourceFile + 'stsbenchmark/'\n",
    "    sts_train = load_sts_dataset(os.path.join(stsbenchmarkDir, \"sts-train.csv\"))    \n",
    "    sts_dev = load_sts_dataset(os.path.join(stsbenchmarkDir, \"sts-dev.csv\"))\n",
    "    sts_test = load_sts_dataset(os.path.join(stsbenchmarkDir, \"sts-test.csv\"))\n",
    "    sts_other = load_sts_dataset(os.path.join(stscompanionDir, \"sts-other.csv\"))\n",
    "    sts_mt = load_sts_dataset(os.path.join(stscompanionDir, \"sts-mt.csv\"))\n",
    "    \n",
    "    sts_all = pd.concat([sts_train, sts_dev, sts_test, sts_other, sts_mt ])\n",
    "    \n",
    "    return sts_all\n",
    "\n",
    "sts_all = load_all_sts_dataset()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_sts_by_year_task():\n",
    "    # Divide STS datasets based on their year and tasks\n",
    "    sts_by_year_task = {}\n",
    "    \n",
    "    for year_task in sts_all['year-task'].unique():\n",
    "        indices = [i for i, x in enumerate(list(sts_all['year-task'])) if x == year_task]\n",
    "        \n",
    "        pairs = sts_all.iloc[indices]\n",
    "        \n",
    "        sts_by_year_task[year_task] = pairs\n",
    "        \n",
    "    return sts_by_year_task\n",
    "\n",
    "sts_by_year_task = load_sts_by_year_task()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_sts_by_year():\n",
    "    # Divide STS datasets ONLY based on their year (different tasks in that year are merged).\n",
    "\n",
    "    sts_by_year = {}\n",
    "    \n",
    "    for year in ['2012', '2013', '2014', '2015', '2016', '2017']:\n",
    "        indices = [i for i, x in enumerate(list(sts_all['year-task'])) if x.startswith(year)]\n",
    "        \n",
    "        pairs = sts_all.iloc[indices]\n",
    "        pairs = pairs.copy()\n",
    "        pairs['year-task'] = year\n",
    "        sts_by_year[year] = pairs\n",
    "        \n",
    "    return sts_by_year\n",
    "\n",
    "sts_by_year_task = load_sts_by_year_task()\n",
    "\n",
    "sts_by_year = load_sts_by_year()\n",
    "\n",
    "\n",
    "filename = resourceFile + '2015-answers-students.test.tsv'\n",
    "sent_pairs = []\n",
    "with tf.io.gfile.GFile(filename, \"r\") as f:\n",
    "    for line in f:\n",
    "        ts = line.strip().split(\"\\t\")\n",
    "        if len(ts) == 3:\n",
    "            sent_pairs.append((ts[1], ts[2], float(ts[0])))\n",
    "answers_students_2015 =  pd.DataFrame(sent_pairs, columns=[\"sent_1\", \"sent_2\", \"sim\"])\n",
    "\n",
    "\n",
    "# show some sample sts data    \n",
    "sts_all[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>sim</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>There is no boy playing outdoors and there is ...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>3.300</td>\n",
       "      <td>NEUTRAL\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>A group of boys in a yard is playing and a man...</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>3.700</td>\n",
       "      <td>NEUTRAL\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>A group of children is playing in the house an...</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>3.000</td>\n",
       "      <td>NEUTRAL\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>A brown dog is attacking another animal in fro...</td>\n",
       "      <td>A brown dog is attacking another animal in fro...</td>\n",
       "      <td>4.900</td>\n",
       "      <td>ENTAILMENT\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>A brown dog is attacking another animal in fro...</td>\n",
       "      <td>A brown dog is helping another animal in front...</td>\n",
       "      <td>3.665</td>\n",
       "      <td>NEUTRAL\\r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  idx                                             sent_1  \\\n",
       "0   6  There is no boy playing outdoors and there is ...   \n",
       "1   7  A group of boys in a yard is playing and a man...   \n",
       "2   8  A group of children is playing in the house an...   \n",
       "3  10  A brown dog is attacking another animal in fro...   \n",
       "4  11  A brown dog is attacking another animal in fro...   \n",
       "\n",
       "                                              sent_2    sim         label  \n",
       "0  A group of kids is playing in a yard and an ol...  3.300     NEUTRAL\\r  \n",
       "1  The young boys are playing outdoors and the ma...  3.700     NEUTRAL\\r  \n",
       "2  The young boys are playing outdoors and the ma...  3.000     NEUTRAL\\r  \n",
       "3  A brown dog is attacking another animal in fro...  4.900  ENTAILMENT\\r  \n",
       "4  A brown dog is helping another animal in front...  3.665     NEUTRAL\\r  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def download_sick(f): \n",
    "\n",
    "    response = requests.get(f).text\n",
    "\n",
    "    lines = response.split(\"\\n\")[1:]\n",
    "    lines = [l.split(\"\\t\") for l in lines if len(l) > 0]\n",
    "    lines = [l for l in lines if len(l) == 5]\n",
    "\n",
    "    df = pd.DataFrame(lines, columns=[\"idx\", \"sent_1\", \"sent_2\", \"sim\", \"label\"])\n",
    "    df['sim'] = pd.to_numeric(df['sim'])\n",
    "    return df\n",
    "    \n",
    "sick_all = download_sick(\"https://raw.githubusercontent.com/alvations/stasis/master/SICK-data/SICK_test_annotated.txt\")\n",
    "\n",
    "sick_all[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    \n",
    "    def __init__(self, sentence):\n",
    "        self.raw = sentence\n",
    "        normalized_sentence = sentence.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "        self.tokens = [t.lower() for t in nltk.word_tokenize(normalized_sentence)]\n",
    "        \n",
    "def run_benchmark(sentences1, sentences2, model_str): \n",
    "    \n",
    "    model = eval(model_str)\n",
    "    embeddings = []\n",
    "    \n",
    "    wv_len = 300\n",
    "    \n",
    "    if 'bert' in model_str:\n",
    "        wv_len = 768\n",
    "        \n",
    "    for (sent1, sent2) in zip(sentences1, sentences2): \n",
    "\n",
    "        tokens1 =  sent1.tokens\n",
    "        tokens2 =  sent2.tokens\n",
    "\n",
    "        tokens1 = [token for token in tokens1 if token in model and token.islower()]\n",
    "        tokens2 = [token for token in tokens2 if token in model and token.islower()]\n",
    "        \n",
    "        if tokens1 == [] and tokens2 != []:\n",
    "            embedding1 = np.zeros(wv_len)\n",
    "            embedding2 = np.average([model[token] for token in tokens2], axis=0)\n",
    "        elif tokens2 == [] and tokens1 != []:\n",
    "            embedding2 = np.zeros(wv_len)\n",
    "            embedding1 = np.average([model[token] for token in tokens1], axis=0)\n",
    "        elif tokens2 != [] and tokens1 != []:     \n",
    "            embedding1 = np.average([model[token] for token in tokens1], axis=0)\n",
    "            embedding2 = np.average([model[token] for token in tokens2], axis=0)\n",
    "        else:\n",
    "            embedding1 = np.zeros(wv_len)\n",
    "            embedding2 = np.zeros(wv_len)\n",
    "\n",
    "#         if isinstance(embedding1, float) or isinstance(embedding2, float):\n",
    "#             embeddings.append(np.zeros(300))\n",
    "#             embeddings.append(np.zeros(300))\n",
    "#         else:\n",
    "#             embeddings.append(embedding1)\n",
    "#             embeddings.append(embedding2)\n",
    "        embeddings.append(embedding1)\n",
    "        embeddings.append(embedding2)\n",
    "\n",
    "\n",
    "    sims = [cosine_similarity(embeddings[idx*2].reshape(1, -1), embeddings[idx*2+1].reshape(1, -1))[0][0] for idx in range(int(len(embeddings)/2))]\n",
    "    return sims\n",
    "\n",
    "def run_experiment(df, benchmarks): \n",
    "    \n",
    "    sentences1 = [Sentence(s) for s in df['sent_1']]\n",
    "    sentences2 = [Sentence(s) for s in df['sent_2']]\n",
    "    \n",
    "    pearson_cors, spearman_cors = [], []\n",
    "    for label, method in benchmarks:\n",
    "        sims = method(sentences1, sentences2)\n",
    "        pearson_correlation = round(scipy.stats.pearsonr(sims, df['sim'])[0] * 100,2)\n",
    "        #print(label, pearson_correlation)\n",
    "        pearson_cors.append(pearson_correlation)\n",
    "        \n",
    "    return pearson_cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\biagi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STS-2012-MSRvid\n",
      "STS-2014-images\n",
      "STS-2015-images\n",
      "STS-2014-deft-forum\n",
      "STS-2012-MSRpar\n",
      "STS-2014-deft-news\n",
      "STS-2013-headlines\n",
      "STS-2014-headlines\n",
      "STS-2015-headlines\n",
      "STS-2016-headlines\n",
      "STS-2017-track5.en-en\n",
      "STS-2015-answers-forums\n",
      "STS-2016-answer-answer\n",
      "STS-2012-surprise.OnWN\n",
      "STS-2013-FNWN\n",
      "STS-2013-OnWN\n",
      "STS-2014-OnWN\n",
      "STS-2014-tweet-news\n",
      "STS-2015-belief\n",
      "STS-2016-plagiarism\n",
      "STS-2016-question-question\n",
      "STS-2012-SMTeuroparl\n",
      "STS-2012-surprise.SMTnews\n",
      "STS-2016-postediting\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "benchmarks = [\n",
    "     (\"orig-glove\", ft.partial(run_benchmark, model_str= 'orig_glove')),\n",
    "    (\"HSR-glove\", ft.partial(run_benchmark, model_str= 'post_glove'))]\n",
    "\n",
    "pearson_results_year_task = {}\n",
    "\n",
    "for year_task in sts_all['year-task'].unique():\n",
    "    print('STS-' + year_task)\n",
    "    pearson_results_year_task['STS-' + year_task] = run_experiment(sts_by_year_task[year_task], benchmarks)  \n",
    "    \n",
    "pearson_results_year_task['SICK'] = run_experiment(sick_all, benchmarks) \n",
    "pearson_results_year_task['2015-answers_students'] = run_experiment(answers_students_2015, benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig-glove</th>\n",
       "      <th>HSR-glove</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>STS-2012-MSRpar</th>\n",
       "      <td>42.05</td>\n",
       "      <td>38.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2012-MSRvid</th>\n",
       "      <td>51.41</td>\n",
       "      <td>50.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2012-surprise.OnWN</th>\n",
       "      <td>54.04</td>\n",
       "      <td>64.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2012-SMTeuroparl</th>\n",
       "      <td>52.71</td>\n",
       "      <td>51.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2012-surprise.SMTnews</th>\n",
       "      <td>44.38</td>\n",
       "      <td>50.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2013-FNWN</th>\n",
       "      <td>35.20</td>\n",
       "      <td>34.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2013-OnWN</th>\n",
       "      <td>45.90</td>\n",
       "      <td>56.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2013-headlines</th>\n",
       "      <td>59.59</td>\n",
       "      <td>66.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2014-OnWN</th>\n",
       "      <td>54.25</td>\n",
       "      <td>63.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2014-deft-forum</th>\n",
       "      <td>25.70</td>\n",
       "      <td>35.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2014-deft-news</th>\n",
       "      <td>60.66</td>\n",
       "      <td>67.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2014-headlines</th>\n",
       "      <td>56.26</td>\n",
       "      <td>61.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2014-tweet-news</th>\n",
       "      <td>57.82</td>\n",
       "      <td>71.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2014-images</th>\n",
       "      <td>51.45</td>\n",
       "      <td>62.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2015-answers-forums</th>\n",
       "      <td>33.28</td>\n",
       "      <td>44.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-answers_students</th>\n",
       "      <td>60.28</td>\n",
       "      <td>68.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2015-belief</th>\n",
       "      <td>36.28</td>\n",
       "      <td>56.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2015-headlines</th>\n",
       "      <td>64.32</td>\n",
       "      <td>70.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2015-images</th>\n",
       "      <td>62.59</td>\n",
       "      <td>68.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SICK</th>\n",
       "      <td>62.11</td>\n",
       "      <td>62.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           orig-glove  HSR-glove\n",
       "STS-2012-MSRpar                 42.05      38.62\n",
       "STS-2012-MSRvid                 51.41      50.77\n",
       "STS-2012-surprise.OnWN          54.04      64.69\n",
       "STS-2012-SMTeuroparl            52.71      51.97\n",
       "STS-2012-surprise.SMTnews       44.38      50.29\n",
       "STS-2013-FNWN                   35.20      34.34\n",
       "STS-2013-OnWN                   45.90      56.34\n",
       "STS-2013-headlines              59.59      66.67\n",
       "STS-2014-OnWN                   54.25      63.09\n",
       "STS-2014-deft-forum             25.70      35.78\n",
       "STS-2014-deft-news              60.66      67.52\n",
       "STS-2014-headlines              56.26      61.09\n",
       "STS-2014-tweet-news             57.82      71.12\n",
       "STS-2014-images                 51.45      62.16\n",
       "STS-2015-answers-forums         33.28      44.12\n",
       "2015-answers_students           60.28      68.41\n",
       "STS-2015-belief                 36.28      56.20\n",
       "STS-2015-headlines              64.32      70.02\n",
       "STS-2015-images                 62.59      68.46\n",
       "SICK                            62.11      62.56"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pearson_results_year_task_df = pd.DataFrame(pearson_results_year_task)\n",
    "pearson_results_year_task_df = pearson_results_year_task_df.transpose()\n",
    "pearson_results_year_task_df = pearson_results_year_task_df.rename(columns={i:b[0] for i, b in enumerate(benchmarks)})\n",
    "\n",
    "pearson_results_year_task_df.reindex(['STS-2012-MSRpar', 'STS-2012-MSRvid', 'STS-2012-surprise.OnWN', 'STS-2012-SMTeuroparl', 'STS-2012-surprise.SMTnews','STS-2013-FNWN', 'STS-2013-OnWN', 'STS-2013-headlines',  'STS-2014-OnWN', 'STS-2014-deft-forum','STS-2014-deft-news', 'STS-2014-headlines', 'STS-2014-tweet-news',  'STS-2014-images', 'STS-2015-answers-forums', '2015-answers_students', 'STS-2015-belief',  'STS-2015-headlines', 'STS-2015-images', 'SICK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.89666666666667"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([35.20,45.90,59.59])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
